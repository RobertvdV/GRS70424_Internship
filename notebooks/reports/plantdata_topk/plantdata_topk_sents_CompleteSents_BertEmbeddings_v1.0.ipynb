{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import warnings\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu =  nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "\n",
    "        cls_hs = self.bert(**kwargs)\n",
    "        hidden_state = cls_hs[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        \n",
    "        x = self.fc1(pooler)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return self.fc1(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Success\n"
     ]
    }
   ],
   "source": [
    "model = BERT(bert)\n",
    "\n",
    "try:\n",
    "    model_save_name = 'saved_weights_BERT_description_classifier.pt'\n",
    "    path = \"../../../models/saved_weights/\" + model_save_name\n",
    "    model.load_state_dict(torch.load(path, \n",
    "                                        map_location=torch.device('cpu')))\n",
    "    print('Local Success')\n",
    "except:\n",
    "    raise ValueError\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(span, model, pred_values=False, truncation=True, threshold=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=truncation)\n",
    "        return model(**inputs)\n",
    "\n",
    "def similarity(GroundTruth, Prediction):\n",
    "    GroundTruth = embeddings(GroundTruth, model=model)\n",
    "    Prediction = embeddings(Prediction, model=model)\n",
    "    return cosine_similarity(GroundTruth, Prediction)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../../../data/interim/\"\n",
    "\n",
    "df_andrei = pd.read_csv(root + \"DF_Andrei.csv\", header=[0, 1], index_col=0) \n",
    "df_pierre = pd.read_csv(root + \"DF_Pierre.csv\", header=[0, 1], index_col=0) \n",
    "df_daniel = pd.read_csv(root + \"DF_Daniel.csv\", header=[0, 1], index_col=0) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../../../data/processed/\"\n",
    "sentences_all = {}\n",
    "\n",
    "f = open(F\"{root}Sentences_Pierre.pkl\", 'rb')\n",
    "sentences_Pierre = pickle.load(f)\n",
    "sentences_all |= sentences_Pierre \n",
    "\n",
    "f = open(F\"{root}Sentences_Andrei.pkl\", 'rb')\n",
    "sentences_Andrei = pickle.load(f)\n",
    "sentences_all |= sentences_Andrei \n",
    "\n",
    "f = open(F\"{root}Sentences_Kissling.pkl\", 'rb')\n",
    "sentences_Kissling = pickle.load(f)\n",
    "sentences_all |= sentences_Kissling \n",
    "\n",
    "# Drop duplicates\n",
    "for species, sentences in sentences_all.items():\n",
    "    sentences_all[species] = list(set(sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/647 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acacia amythethophylla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/647 [00:26<4:47:51, 26.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acacia ataxacantha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/647 [01:06<11:51:50, 66.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/robertvandevlasakker/Documents/Wageningen University/Period 0 - GRS 70424 Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m top_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tqdm(sentences, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSentences\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     gt_sim \u001b[39m=\u001b[39m similarity(df_sent, sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m gt_sim:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         sentence \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mNaN\n",
      "\u001b[1;32m/Users/robertvandevlasakker/Documents/Wageningen University/Period 0 - GRS 70424 Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb Cell 10\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(GroundTruth, Prediction)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity\u001b[39m(GroundTruth, Prediction):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     GroundTruth \u001b[39m=\u001b[39m embeddings(GroundTruth, model\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     Prediction \u001b[39m=\u001b[39m embeddings(Prediction, model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cosine_similarity(GroundTruth, Prediction)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m/Users/robertvandevlasakker/Documents/Wageningen University/Period 0 - GRS 70424 Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb Cell 10\u001b[0m in \u001b[0;36membeddings\u001b[0;34m(span, model, pred_values, truncation, threshold)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(span, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39mtruncation)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/robertvandevlasakker/Documents/Wageningen University/Period 0 - GRS 70424 Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb Cell 10\u001b[0m in \u001b[0;36mBERT.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     cls_hs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     hidden_state \u001b[39m=\u001b[39m cls_hs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robertvandevlasakker/Documents/Wageningen%20University/Period%200%20-%20GRS%2070424%20Internship/notebooks/reports/plantdata_topk/plantdata_topk_sents_CompleteSents_BertEmbeddings_v1.0.ipynb#X55sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     pooler \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:567\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    568\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    569\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    570\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    574\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:345\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    343\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 345\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    346\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:299\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    296\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa_layer_norm(sa_output \u001b[39m+\u001b[39m x)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m ffn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffn(sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    300\u001b[0m ffn_output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer_norm(ffn_output \u001b[39m+\u001b[39m sa_output)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    302\u001b[0m output \u001b[39m=\u001b[39m (ffn_output,)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:244\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_chunking_to_forward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, \u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:247\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mff_chunk\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 247\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin1(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    248\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[1;32m    249\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin2(x)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k= 5\n",
    "google_form_lst = []\n",
    "\n",
    "for idx, (species, sentences) in enumerate(tqdm(sentences_all.items())):\n",
    "\n",
    "    print(species)\n",
    "\n",
    "    if idx > 1:\n",
    "        continue\n",
    "    # if species != 'Dypsis thiryana':\n",
    "    #     continue\n",
    "    \n",
    "    df_select = [df_andrei, df_pierre, df_daniel]\n",
    "    if species in df_andrei.index:\n",
    "        df_select = df_select[0]\n",
    "    elif species in df_pierre.index:\n",
    "        df_select = df_select[1]\n",
    "    elif species in df_daniel.index:\n",
    "        df_select = df_select[2]\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    for gt_main_trait in df_select.columns.get_level_values(0).unique():\n",
    "        if gt_main_trait == 'Measurement':\n",
    "            df_subset = df_select[df_select.index == species][gt_main_trait]\n",
    "            # Not really efficient, use PD?\n",
    "    \n",
    "            subtraits = list(df_subset.columns)\n",
    "            values = df_subset.values[0]\n",
    "            for sub_trait, value in zip(subtraits, values):\n",
    "\n",
    "                df_sent  = F\"{sub_trait}: {value}\"\n",
    "\n",
    "                top_list = []\n",
    "\n",
    "                for sentence in tqdm(sentences, leave=False, desc=\"Sentences\"):\n",
    "                    gt_sim = similarity(df_sent, sentence)\n",
    "                    if not gt_sim:\n",
    "                        sentence = np.NaN\n",
    "                    top_list.append((gt_sim, sentence))\n",
    "\n",
    "                top_list.sort(reverse=True)\n",
    "                # top_k_list = [sentence for (_, sentence) in top_list[0:k]]\n",
    "                gt_sim_sum = 0\n",
    "                top_k_list = []\n",
    "                for (gt_sim, sentence) in top_list[0:k]:\n",
    "                    gt_sim_sum += gt_sim\n",
    "                    top_k_list.append(sentence)\n",
    "\n",
    "                # Non Nan for normalization\n",
    "                None_NaNs = k - top_k_list.count(np.NaN) + 2e-26 # Float division               \n",
    "\n",
    "                google_form_lst.append((species, gt_main_trait, sub_trait, [df_sent], *top_k_list, df_sent.capitalize(), gt_sim_sum/None_NaNs))\n",
    "        else:\n",
    "            df_subset = df_select[df_select.index == species][gt_main_trait]\n",
    "            present_traits = df_subset.loc[:, df_subset.any()].columns.values\n",
    "            \n",
    "            # df_sent  = ' '.join(gt_main_trait + ' ' + present_traits)\n",
    "            \n",
    "            # NEW VERSION SINGLE TRAIT\n",
    "            size = present_traits.shape\n",
    "            if not size[0]:\n",
    "                continue\n",
    "            df_sent = F\"{gt_main_trait} {present_traits[0]}\"\n",
    "            # print(df_sent)\n",
    "\n",
    "            top_list = []\n",
    "\n",
    "            for sentence in tqdm(sentences, leave=False, desc=\"Sentences\"):\n",
    "                gt_sim = similarity(df_sent, sentence)\n",
    "\n",
    "                if not gt_sim:\n",
    "                    sentence = np.NaN\n",
    "\n",
    "                # print(gt_sim, sentence, df_sent)\n",
    "                top_list.append((gt_sim, sentence))\n",
    "\n",
    "            top_list.sort(reverse=True)\n",
    "\n",
    "            # print(top_list)\n",
    "\n",
    "            # top_k_list = [sentence for (_, sentence) in top_list[0:k]]\n",
    "            gt_sim_sum = 0\n",
    "            top_k_list = []\n",
    "            for (gt_sim, sentence) in top_list[0:k]:\n",
    "                gt_sim_sum += gt_sim\n",
    "                top_k_list.append(sentence)\n",
    "\n",
    "            # Non Nan for normalization\n",
    "            None_NaNs = k - top_k_list.count(np.NaN) + 2e-26 # Float division       \n",
    "\n",
    "            # GoogleSent = gt_main_trait + ': ' + ', '.join(list(present_traits))\n",
    "            GoogleSent = F\"{gt_main_trait}: {present_traits[0]}\"\n",
    "            google_form_lst.append((species, gt_main_trait, gt_main_trait, list(present_traits), *top_k_list, GoogleSent.capitalize(), gt_sim_sum/None_NaNs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51432b8e5767c06330d9b51dfad63f9db0ea39868e37d921b9c2e277373f8d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
