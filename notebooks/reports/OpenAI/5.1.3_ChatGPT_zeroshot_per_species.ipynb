{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.environ['OPENAI_API']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_present_traits(species, df):\n",
    "\n",
    "    s = df.loc[species]\n",
    "    GT_traits = list(s.where(s == 1).dropna().index)\n",
    "    empty_traits = [F\"{Entity}:\" for (Entity, Value) in GT_traits]\n",
    "\n",
    "    return GT_traits, empty_traits\n",
    "\n",
    "def create_trait_dict(multi_index):\n",
    "    # Create an empty dictionary\n",
    "    result_dict = {}\n",
    "\n",
    "    # Iterate through the MultiIndex object\n",
    "    for idx in multi_index:\n",
    "        key = idx[0]\n",
    "        value = idx[1]\n",
    "        if key not in result_dict:\n",
    "            result_dict[key] = []\n",
    "        result_dict[key].append(value)\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def combine_words_with_capital(string):\n",
    "    # remove non-alphanumeric characters\n",
    "    string = re.sub(r'[^\\w\\s/]', '', string)\n",
    "    # split the string on the slash (\"/\")\n",
    "    parts = string.split('/')\n",
    "    # combine words with capitalization for each part\n",
    "    parts = [''.join(word.capitalize() for word in part.split()) for part in parts]\n",
    "    # join the parts with an empty string\n",
    "    return ''.join(parts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trait Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_traits = \"../../../data/OpenAI/Traits/\"\n",
    "\n",
    "with open(F\"{folder_traits}Andrei.json\", 'r') as f:\n",
    "  caribbean_traits_dict = json.load(f)\n",
    "\n",
    "with open(F\"{folder_traits}Andrei.json\", 'r') as f:\n",
    "  caribbean_traits_dict = json.load(f)\n",
    "\n",
    "with open(F\"{folder_traits}Andrei.json\", 'r') as f:\n",
    "  caribbean_traits_dict = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../../../data/OpenAI/DataFrames/\"\n",
    "\n",
    "file = \"DF_Andrei.csv\"\n",
    "df_Andrei = pd.read_csv(F\"{root}{file}\", header=[0, 1], index_col=0)\\\n",
    "    .rename_axis('Species', axis='index')\n",
    "df_Andrei_species = list(df_Andrei.index)\n",
    "\n",
    "\n",
    "file = \"DF_Daniel.csv\"\n",
    "df_Daniel = pd.read_csv(F\"{root}{file}\", header=[0, 1], index_col=0)\\\n",
    "    .rename_axis('Species', axis='index')\\\n",
    "    .dropna()\n",
    "df_Daniel_species = list(df_Daniel.index)\n",
    "\n",
    "file = \"DF_Pierre.csv\"\n",
    "df_Pierre = pd.read_csv(F\"{root}{file}\", header=[0, 1], index_col=0)\\\n",
    "    .rename_axis('Species', axis='index')\n",
    "df_Pierre_species = list(df_Pierre.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "# Define the path to the directory where the prompts and results will be saved\n",
    "folder_prompts = \"../../../data/OpenAI/PromptsResults/ZeroShot/\"\n",
    "\n",
    "species_lst = df_Andrei.index\n",
    "trait_dict = create_trait_dict(df_Andrei)\n",
    "traits = list(trait_dict.keys())\n",
    "\n",
    "# Loop over species\n",
    "for idx, species in enumerate(species_lst[0:]):\n",
    "   \n",
    "    # Replace spaces in the species name with underscores\n",
    "    folder_species = species.replace(' ', '_')\n",
    "\n",
    "    # Try to create a directory for the prompts for the species\n",
    "    try:\n",
    "        os.makedirs(F\"{folder_prompts}{folder_species}\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    for trait in (pbar := tqdm(traits[0:], leave=False, position=0)):\n",
    "        pbar.set_description(f\"{idx}: {species}\")\n",
    "\n",
    "        # Get the options for the trait\n",
    "        options = trait_dict[trait]\n",
    "\n",
    "        # Create the question and options for the ChatGPT prompt\n",
    "        question = F\"For the species '{species}' which of the following options is applicable for the trait '{trait}'\" \n",
    "        text_helper = F\"Return 'NaN' if none of the options are applicable. Please return your answer as a Python list.\"\n",
    "        user_content = F\"{question}\\n{options}\\n{text_helper}\"\n",
    "\n",
    "        # Combine the words in the trait name with capital letters and use this as the file name\n",
    "        file_name = combine_words_with_capital(trait)\n",
    "        \n",
    "        # Check if file is already there (OpenAI Outage)\n",
    "        if os.path.exists(F\"{folder_prompts}{folder_species}/{file_name}.json\"):\n",
    "            continue\n",
    "\n",
    "        # # Create the messages to send to the ChatGPT API\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "        # Call the ChatGPT API to generate a completion for the prompt\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "        )\n",
    "\n",
    "        # Lower pressure on API?\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Save the completion to a JSON file with the file name in the directory\n",
    "        with open(F\"{folder_prompts}{folder_species}/{file_name}.json\", 'w') as fp:\n",
    "            json.dump(completion, fp)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
